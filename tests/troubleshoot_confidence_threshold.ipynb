{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Iterate over detections and extract faces based on confidence threshold\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detection \u001b[38;5;129;01min\u001b[39;00m detections:\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m detection[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.4\u001b[39m:  \u001b[38;5;66;03m# Confidence threshold\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] Detection confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetection[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Adding face to the list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# Extract bounding box coordinates\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "session = ort.InferenceSession(\"/home/el02/PiPresence/data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "image = cv2.imread(\"/home/el02/PiPresence/data/test_images/common.jpg\")\n",
    "\n",
    "# Preprocess image for ONNX model\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "# Resize image to the required size for YOLO model\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "# Normalize pixel values to range [0, 1]\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "# Change image layout to channel-first format as required by ONNX model\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel first\n",
    "# Add batch dimension (needed by the model)\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference on the provided image\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "outputs = session.run(None, {input_name: input_image})\n",
    "detections = outputs[0]\n",
    "faces = []\n",
    "\n",
    "# Iterate over detections and extract faces based on confidence threshold\n",
    "for detection in detections:\n",
    "    if detection[4] > 0.4:  # Confidence threshold\n",
    "        print(f\"[DEBUG] Detection confidence: {detection[4]} - Adding face to the list\")\n",
    "        # Extract bounding box coordinates\n",
    "        x1, y1, x2, y2 = int(detection[0]), int(detection[1]), int(detection[2]), int(detection[3])\n",
    "        # Crop the detected face from the original image\n",
    "        face = image[y1:y2, x1:x2]\n",
    "        faces.append(face)\n",
    "    else:\n",
    "        print(f\"[DEBUG] Detection confidence too low: {detection[4]} - Ignoring\")\n",
    "print(faces, detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming outputs[0] has a shape of (1, 84, 8400)\n",
    "output_tensor = outputs[0][0]  # Shape: (1, 84, 8400)\n",
    "output_tensor = np.squeeze(output_tensor)  # Remove batch dimension, Shape: (84, 8400)\n",
    "\n",
    "# Extract bounding box predictions (first 4 values)\n",
    "bboxes = output_tensor[:4, :]  # Shape: (4, 8400)\n",
    "\n",
    "# Extract objectness scores (5th value, index 4)\n",
    "objectness_scores = output_tensor[4, :]  # Shape: (8400,)\n",
    "\n",
    "# Extract class scores (remaining 80 values, from index 5 to 84)\n",
    "class_scores = output_tensor[5:, :]  # Shape: (80, 8400)\n",
    "\n",
    "# Compute the maximum class score for each prediction\n",
    "max_class_scores = np.max(class_scores, axis=0)  # Shape: (8400,)\n",
    "\n",
    "# Compute the final confidence score for each of the 8400 predictions\n",
    "confidence_scores = objectness_scores * max_class_scores  # Shape: (8400,)\n",
    "\n",
    "confidence_threshold = 0.5\n",
    "selected_indices = confidence_scores > confidence_threshold\n",
    "\n",
    "# Filtered bounding boxes, confidence scores, and class indices\n",
    "filtered_bboxes = bboxes[:, selected_indices]  # Shape: (4, number of selected predictions)\n",
    "filtered_confidences = confidence_scores[selected_indices]  # Shape: (number of selected predictions)\n",
    "filtered_class_indices = np.argmax(class_scores[:, selected_indices], axis=0)  # Shape: (number of selected predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Preprocessing image for face detection\n",
      "[INFO] Running inference on the image\n",
      "[INFO] Detection: Confidence 31.12, Box: [5, 22, 24, 26]\n",
      "[INFO] Detection: Confidence 14.84, Box: [16, 15, 16, 16]\n",
      "[INFO] Detection: Confidence 64.00, Box: [11, 45, 49, 53]\n",
      "[INFO] Detection: Confidence 29.83, Box: [32, 31, 33, 33]\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n",
      "[DEBUG] Detection confidence too low: 0.00 - Ignoring\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the model and prepare image for inference\n",
    "session = ort.InferenceSession(\"/home/el02/PiPresence/data/models/yolov8n.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "image = cv2.imread(\"/home/el02/PiPresence/data/test_images/common.jpg\")\n",
    "\n",
    "# Preprocess the image\n",
    "print(\"[INFO] Preprocessing image for face detection\")\n",
    "input_image = cv2.resize(image, (640, 640))\n",
    "input_image = input_image.astype(np.float32) / 255.0\n",
    "input_image = np.transpose(input_image, (2, 0, 1))  # Channel-first format\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# Perform inference\n",
    "print(\"[INFO] Running inference on the image\")\n",
    "outputs = session.run(None, {input_name: input_image})\n",
    "detections = outputs[0][0]  # Remove batch dimension\n",
    "\n",
    "# Set confidence threshold\n",
    "confidence_threshold = 0.4\n",
    "\n",
    "# Iterate over detections and draw bounding boxes\n",
    "for detection in detections:\n",
    "    confidence = detection[4]\n",
    "    if confidence > confidence_threshold:\n",
    "        # Extract bounding box coordinates\n",
    "        x1, y1, x2, y2 = int(detection[0]), int(detection[1]), int(detection[2]), int(detection[3])\n",
    "        \n",
    "        # Draw bounding box on the image\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw the confidence score label\n",
    "        label = f\"Conf: {confidence:.2f}\"\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        print(f\"[INFO] Detection: Confidence {confidence:.2f}, Box: [{x1}, {y1}, {x2}, {y2}]\")\n",
    "    else:\n",
    "        print(f\"[DEBUG] Detection confidence too low: {confidence:.2f} - Ignoring\")\n",
    "\n",
    "# Show the resulting image with detections\n",
    "cv2.imshow(\"Detections\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'data/models/yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.25M/6.25M [00:00<00:00, 12.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.29 🚀 Python-3.10.12 torch-2.5.1+cu124 CPU (AMD Ryzen 9 7845HX with Radeon Graphics)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary (fused): 168 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'data/models/yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.37...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 0.4s, saved as 'data/models/yolov8n.onnx' (12.2 MB)\n",
      "\n",
      "Export complete (0.6s)\n",
      "Results saved to \u001b[1m/home/el02/PiPresence/tests/data/models\u001b[0m\n",
      "Predict:         yolo predict task=detect model=data/models/yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=data/models/yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/models/yolov8n.onnx'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a YOLOv8 model from a .pt file\n",
    "model = YOLO('../data/models/yolov8n.pt')  # Change 'yolov8n.pt' to your specific .pt file path\n",
    "\n",
    "# Export the model to ONNX format\n",
    "model.export(format='onnx', imgsz=(640, 640))  # Set imgsz to desired resolution, default is usually (640, 640)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is correctly exported and valid.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the exported ONNX model\n",
    "onnx_model_path = 'data/models/yolov8n.onnx'\n",
    "model = onnx.load(onnx_model_path)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "print(\"The model is correctly exported and valid.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
